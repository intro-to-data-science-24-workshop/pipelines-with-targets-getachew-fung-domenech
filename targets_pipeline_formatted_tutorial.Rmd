---
title: "Establishing Pipelines With Targets"
subtitle: "Reproducible and Efficient Workflows in R"
output: 
  rmdformats::robobook:
  toc: TRUE
  toc_float: TRUE
  highlight: tango
author: "Group E: Laia Domenech Burin, Hanna Fantahun Getachew and Chloe Fung"
date: "`r format(Sys.time(), '%B %d, %Y | %H:%M:%S | %Z')`"
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      error = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)

suppressMessages(suppressWarnings({
  library(targets)
  library(tidymodels)
  library(tidyverse)
  library(discrim)
}))

library(targets)
library(tidyverse)
library(tidymodels)
library(discrim)
library(ggplot2)

```

## Introduction

This document provides a step-by-step guide to **establishing reproducible workflows** using the `targets` package in R. By following this tutorial, you'll learn how to efficiently manage data processing with **pipelines** and compare the outcomes of analyses conducted with and without targets.

We will use the built-in `iris` dataset as a demonstration.

### Why Use Pipelines?

Pipelines enable **workflow automation** by caching intermediate results and selectively rerunning only the modified parts of your analysis. This approach enhances **efficiency**, **reproducibility**, and **scalability**, ensuring a smoother and more streamlined process.

## Defining a Simple Pipeline

We'll create a pipeline to *explore* the variables of the `iris` dataset, and create a *classification* *model* that predicts the species based on all the other variables.

### Pipeline Setup (`_targets.R`)

To build our setup, we will use the function `use_targets()`. This automatically creates a template `_targets.R` file that we can use to introduce our functions into the whole workflow.

```{r eval=FALSE}
use_targets()
```

As a next step, we will create a `functions.R` file in the sub-folder `R` that contains all the functions we use:

-   for descriptive statistics: `summary_statistics()` and `plot_histogram()`
-   training and fitting the model: `split_train_test_data()`, `build_and_fit_model()`
-   using the model to predict new observations and evaluate: `predict_model_fit()` and `model_evaluate_metrics()`.

```{r load-code}
raw_code <- readLines("./R/functions.R")

cat(paste(raw_code, collapse = "\n"))
```

Now that we have all our functions that make the pipeline work defined, we only need to include them in the `_targets.R` file! The file has the following structure:

```{r, eval= FALSE}
library(targets)

# Set target options:
tar_option_set(
  packages = c("tibble")
)

tar_source()

list(
  tar_target(
    name = data,
    command = tibble(x = rnorm(100), y = rnorm(100))
  ),
  tar_target(
    name = model,
    command = coefficients(lm(y ~ x, data = data))
  )
)
```

The **key components** of this structure are the following:

-   `library(targets)` initializes the package
-   `tar_options_set()` defines global options for the pipeline, such as the required packages (in this case, `tibble`).
-   `tar_source()` automatically loads the R functions defined in the `R/functions.R` file, keeping your pipeline modular and organized.
-   a `list()` that contains a series of `tar_target()` functions, where:
    -   `name`: Specifies the name of the object generated at this step.
    -   `command`: Contains the code or function that will be executed to generate the corresponding target.

In this example, the pipeline creates:

-   A `data` target containing a tibble of random values.
-   A `model` target storing the coefficients of a linear regression model based on data.

Each `tar_target()` represents a step in the pipeline, ensuring that these steps are only recomputed when necessary.

We will modify the content of the template according to our functions.

```{r}
raw_targets_code <- readLines("_targets.R")

cat(paste(raw_targets_code, collapse = "\n"))
```

Can you identify what each step does?

## Running the Pipeline

Once your pipeline is fully defined in `_targets.R`, you can run it using the following command:

```{r message=FALSE, warning=FALSE}
tar_make()
```

Did this work? Let's check with the function `tar_manifest()`, lists verbose[^1] information about each target.

[^1]: In programming, if a function or tool runs in "verbose mode," it provides detailed output about what it's doing, often including steps, statuses, and any underlying operations. This is helpful for understanding the process or diagnosing issues.

```{r message=FALSE, warning=FALSE}
tar_manifest(fields = all_of("command"))
```

## Visualizing the Pipeline

You can visualize the dependency graph of your pipeline with `tar_visnetwork()`.

```{r message=FALSE, warning=FALSE}
tar_visnetwork()
```

## Viewing Results

We can access the results of any target using `tar_read()`. For example, if we want to take a look into the exploratory part:

```{r}
tar_read(histogram)
```

How did the model perform? Let's check the evaluation metrics.

```{r}
tar_read(evaluation_metrics)
```

Note that this target contains two outputs! The confusion matrix and overall metrics of accuracy and KAP. We can select items of any target subsetting by index.

```{r}
tar_read(evaluation_metrics)[[2]]
```

We could even perform operations of code on our targets without adding them to the pipeline by selecting them. For example, if we wanted to format our confusion matrix as a plot we could do the following.

You can access the results of any target using `tar_read()`.

```{r}
as_tibble(tar_read(evaluation_metrics)[[2]]$table) %>%
  ggplot(aes(x = Prediction, y = Truth, fill = n)) +
  geom_tile() +
  geom_text(aes(label = n), color = "white", size = 5) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix", x = "Predicted", y = "Actual") +
  theme_minimal()


```

This revised version improves readability, adds clarity, and ensures a smooth flow throughout the document. It also emphasizes key steps, making it easier to follow for someone new to using the `targets` package.

## Comparing Outcomes: With and Without `targets`

Let's compare how the workflow looks with and without using the `targets` package.

### Without `targets`:

When operating without the `targets` package, we typically execute all steps sequentially in a more linear fashion. Below is an example of how we would do our previous `iris` pipeline:

```{r}
data <- iris

data_long <- data %>%
  pivot_longer(cols = -Species, names_to = "Variable", values_to = "Value")
  
ggplot(data_long, aes(x = Value, fill = Species)) +
    geom_histogram(bins = 30, position = "identity", alpha = 0.7, color = "black") +
    facet_wrap(~ Variable, scales = "free") +
    labs(title = "Histograms of Iris Dataset Variables Colored by Species",
         x = "Value",
         y = "Count") +
    theme_minimal() +
    theme(legend.position = "top")

set.seed(123) 

split <- initial_split(data, prop = 0.8, strata = Species)
data_train <- training(split)
data_test  <- testing(split)


model <-  naive_Bayes() %>%
  set_engine("naivebayes") %>%
  set_mode("classification")
  
recipe <- recipe(Species ~ ., data = data_train)
  
workflow <- workflow() %>%
  add_model(model) %>%
  add_recipe(recipe)
  
fit <- workflow %>%
  fit(data = data_train)


model_predictions <- predict(fit, data_test) %>%
    bind_cols(data_test)

metrics <- model_predictions %>%
    metrics(truth = Species, estimate = .pred_class)
  
conf_mat <- model_predictions %>%
    conf_mat(truth = Species, estimate = .pred_class)

```

-   This approach requires executing each step in sequence, making it harder to manage intermediate results. As a result, any changes in the analysis (e.g., modifying the model or data processing steps) necessitate re-running the entire workflow.

-   Without `targets`, it's easy to lose track of which analyses have been executed or to overlook necessary updates to intermediate steps. This can lead to inconsistencies and longer execution times.

In contrast, using the `targets` package allows us to modularize our workflow, cache results, and focus only on the components that have changed, significantly enhancing efficiency and reproducibility.

## Conclusion

We demonstrated how to define a simple pipeline utilizing the `iris` dataset, encompassing essential steps from data preparation to model evaluation. Key components of the pipeline included the creation of `targets`, modular function organization, and visualization of the workflow, which together contribute to an efficient data science process.

Using `tar_make()`, we were able to execute our pipeline, ensuring that computations were only performed when necessary. Furthermore, by leveraging functions like `tar_read()` and `tar_visnetwork()`, we accessed our results easily and visualized the dependencies within our workflow, aiding in better understanding and communication of our analysis.

In summary, the `targets` package equips us with powerful tools to build scalable, reproducible data workflows in R, making it an invaluable asset for data scientists and analysts. We encourage you to experiment with your own datasets and analyses, utilizing these concepts to foster reproducibility and efficiency in your work.
